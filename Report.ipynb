{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Project\n",
    "\n",
    "The goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "I tried the second version. The second version contains 20 identical agents, each with its own copy of the environment.\n",
    "\n",
    "## Learing Algorithm\n",
    "\n",
    "The second version of the environment has 20 different agents. Because all of the agents perform the same task under the same conditions, I've used a signle brain to control 20 agents. The action space is continuous and the `value-based method` like DQN is not suitable in this environment. So I decided to use the `policy-based method`.\n",
    "\n",
    "Policy-based methods offer practical ways of dealing with large actions spaces, even continuous spaces with an infinite number of actions. Instead of computing learned probabilities for each of the many actions, we instead learn statistics of the probability distribution. \n",
    "\n",
    "I trained the network using `Deep Deterministic Policy Gradients(DDGP)` algorithm. It is an `Actor-Critic method` in which two architectures are combined. Actor determines the current policy in continuous space and Critic learns the Q-values in a given (state, action) pair. \n",
    "The Actor and Critic network has three layers with 400 and 300 units in hidden layers and the 33 dimensional vector is used as the input. The final layer of Actor has 4 actions and the final layer of Critic is fully connected with the size of 1.\n",
    "\n",
    "DDPG Hyperparameters:\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 1e-5        # L2 weight decay\n",
    "```\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "The agent was able to receive an average reward (over 100 episodes, and over all 20 agents) of at least +30.\n",
    "![rewards](rewards.png)\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "* add batch normalization to actor networks\n",
    "* try a different learning algorithm like D4PG, PPO or A3C.\n",
    "* immplement Prioritized Experience Replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
